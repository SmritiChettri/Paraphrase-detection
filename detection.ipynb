{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 24 13:47:51 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 556.13                 Driver Version: 556.13         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   37C    P8              6W /   30W |       0MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VICTUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\VICTUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VICTUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset......\n",
      "   id  qid1  qid2                                          question1  \\\n",
      "0   0     1     2  What is the step by step guide to invest in sh...   \n",
      "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "2   2     5     6  How can I increase the speed of my internet co...   \n",
      "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
      "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
      "\n",
      "                                           question2  is_duplicate  \n",
      "0  What is the step by step guide to invest in sh...             0  \n",
      "1  What would happen if the Indian government sto...             0  \n",
      "2  How can Internet speed be increased by hacking...             0  \n",
      "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
      "4            Which fish would survive in salt water?             0  \n",
      "Total samples in dataset: 404348\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset......\") #displaying dataset\n",
    "data = pd.read_csv('questions.csv')\n",
    "data = data.dropna(subset=[\"question1\", \"question2\"])\n",
    "print(data.head())\n",
    "print(f\"Total samples in dataset: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Text......\n"
     ]
    }
   ],
   "source": [
    "#preprocessing the text\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    elif isinstance(text, float):\n",
    "        if text.is_integer():\n",
    "          return str(int(text))\n",
    "        else:\n",
    "          return \"\"\n",
    "    else:\n",
    "      tokens = nltk.word_tokenize(text.lower())\n",
    "      stop_words = set(stopwords.words(\"english\"))\n",
    "      filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "      return \" \".join(filtered_tokens)\n",
    "print(\"Preprocessing Text......\")\n",
    "data[\"question1\"] = data[\"question1\"].apply(preprocess_text)\n",
    "data[\"question2\"] = data[\"question2\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#vectorizing text\n",
    "vectorizer = TfidfVectorizer(max_features = 10000)\n",
    "question1_vectors = vectorizer.fit_transform(data[\"question1\"])\n",
    "question2_vectors = vectorizer.transform(data[\"question2\"])\n",
    "\n",
    "# combining question 1 and question 2 vectors\n",
    "x = hstack((question1_vectors, question2_vectors))\n",
    "\n",
    "y = data[\"is_duplicate\"].values\n",
    "\n",
    "\n",
    "# train-test splitting\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized Sentence1: (404348, 10000)\n",
      "Vectorized Sentence2: (404348, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vectorized Sentence1: {question1_vectors.shape}\")\n",
    "print(f\"Vectorized Sentence2: {question2_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404348, 20000)\n",
      "(404348,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model......\n",
      "Training Model......\n",
      "Epoch 1/50\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 379ms/step - accuracy: 0.7107 - loss: 0.5678 - val_accuracy: 0.7902 - val_loss: 0.4585\n",
      "Epoch 2/50\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 373ms/step - accuracy: 0.8178 - loss: 0.3969 - val_accuracy: 0.8051 - val_loss: 0.4279\n",
      "Epoch 3/50\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 335ms/step - accuracy: 0.8652 - loss: 0.3020 - val_accuracy: 0.8130 - val_loss: 0.4479\n",
      "Epoch 4/50\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 328ms/step - accuracy: 0.8956 - loss: 0.2449 - val_accuracy: 0.8174 - val_loss: 0.4791\n",
      "Epoch 5/50\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 330ms/step - accuracy: 0.9125 - loss: 0.2112 - val_accuracy: 0.8181 - val_loss: 0.4808\n",
      "Epoch 6/50\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 329ms/step - accuracy: 0.9245 - loss: 0.1853 - val_accuracy: 0.8197 - val_loss: 0.4874\n",
      "Epoch 7/50\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 330ms/step - accuracy: 0.9339 - loss: 0.1655 - val_accuracy: 0.8196 - val_loss: 0.4988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d7084fcda0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Building a model(feed-forward neural network)// later try using transformer varient\n",
    "print(\"Building Model......\")\n",
    "model = Sequential([\n",
    "    Dense(512, activation = \"relu\", input_shape = (x_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation = \"relu\"),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(128, activation = \"relu\"),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation = \"relu\"),\n",
    "    Dense(32, activation = \"relu\"),\n",
    "    Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# training a model\n",
    "print(\"Training Model......\")\n",
    "model.fit(x_train, y_train, batch_size=256, epochs=50, validation_data=(x_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.8050698652157784}\n"
     ]
    }
   ],
   "source": [
    "print({accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2528/2528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8050698652157784.2f\n",
      "Model saved successfully as paraphrase_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "y_pred = (model.predict(x_test) > 0.5).astype(\"int\").flatten()\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}.2f\")\n",
    "\n",
    "# Saving trained model\n",
    "model.save(\"paraphrase_model.h5\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")# saving the trained TF-IDF vectorizer\n",
    "\n",
    "print(\"Model saved successfully as paraphrase_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Question1: government regulate internet based services ?\n",
      "Input Question2: internet governance us-centric ?\n",
      "Actual: 0, Predicted: 0\n",
      "Input Question1: long take get offer ?\n",
      "Input Question2: long take get offer netflix ?\n",
      "Actual: 1, Predicted: 1\n",
      "Input Question1: happens fall love professor ?\n",
      "Input Question2: professor , happens fall love student ?\n",
      "Actual: 0, Predicted: 0\n",
      "Input Question1: come family friends ?\n",
      "Input Question2: come friends ?\n",
      "Actual: 1, Predicted: 1\n",
      "Input Question1: tree data structure ?\n",
      "Input Question2: b * tree data structure ?\n",
      "Actual: 0, Predicted: 0\n",
      "Input Question1: illegal report crime ?\n",
      "Input Question2: legal report crime ?\n",
      "Actual: 0, Predicted: 0\n",
      "Input Question1: happen india pakistan gets war win ?\n",
      "Input Question2: war occurs india pakistan effect rest country ?\n",
      "Actual: 1, Predicted: 0\n",
      "Input Question1: good solar panel installation provider templeton , california ca ?\n",
      "Input Question2: good solar panel installation provider terra bella , california ca ?\n",
      "Actual: 1, Predicted: 1\n",
      "Input Question1: displacement nth second= displacement/ time ?\n",
      "Input Question2: assume displacement due transverse load negative second differential term ?\n",
      "Actual: 0, Predicted: 0\n",
      "Input Question1: increase growth hormones ?\n",
      "Input Question2: increase growth hormone 19 ?\n",
      "Actual: 1, Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "for idx, (actual, predicted) in enumerate(zip(y_test[:10], y_pred[:10])):  # Display first 10 samples\n",
    "    question1 = data.iloc[x_test.indices[idx]][\"question1\"]\n",
    "    question2 = data.iloc[x_test.indices[idx]][\"question2\"]\n",
    "\n",
    "    print(f\"Input Question1: {question1}\")\n",
    "    print(f\"Input Question2: {question2}\")\n",
    "    print(f\"Actual: {actual}, Predicted: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
